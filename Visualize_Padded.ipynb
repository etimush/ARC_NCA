{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from NCA import *\n",
    "import arc_agi_utils as aau\n",
    "import cv2\n",
    "import vft\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils import make_path"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Make path for videos",
   "id": "71ed51de60ffef90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "path_video = \"Saved_frames\"\n",
    "make_path(path_video)"
   ],
   "id": "140fceda7b51ebec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Video making functions",
   "id": "c7c693b2e7d4ca04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def write_frame(x, path, frame_number, height, width, chn):\n",
    "    image_np = x.clone().detach().cpu().permute(0,3,2,1).numpy().clip(0,1)[0,:,:,:3]\n",
    "    plt.imsave(f\"{path}/frame_{frame_number}.png\", image_np)\n",
    "\n",
    "def make_video(path, total_frames, height, width, vid_num = \"r\"):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(path+'/' +vid_num+'.mp4', fourcc, 15.0, (height, width))\n",
    "    for frame_number in range(total_frames):\n",
    "       frame_path = path+f\"/frame_{frame_number}.png\"\n",
    "       frame = cv2.imread(frame_path)\n",
    "       #frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "       frame = cv2.flip(frame,1)\n",
    "       frame = cv2.rotate(frame,cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "       frame = cv2.resize(frame, (height, width), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "       out.write(frame)\n",
    "    out.release()"
   ],
   "id": "1854abe6082dc8ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CA settings",
   "id": "89da0c562b313c4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DEVICE = vft.DEVICE\n",
    "CHANNELS = vft.CHANNELS\n",
    "BATCH_SIZE = vft.BATCH_SIZE\n",
    "MASKING = vft.MASKING\n",
    "GENESIZE = vft.GENESIZE"
   ],
   "id": "7268acfeee0705d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load ARC-AGI",
   "id": "53fb3b361f64d7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_path = \"ArcData/data/training\"\n",
    "eval_path = \"ArcData/data/evaluation\"\n",
    "(inputs, outputs), (eval_in, eval_out)= aau.import_data(training_path)\n",
    "\n",
    "max_train = aau.max_n_colors(inputs, outputs)\n",
    "max_eval = aau.max_n_colors(eval_in, eval_out)\n",
    "max_colors = max(max_train, max_eval) +1"
   ],
   "id": "6a0999bb4003a3b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Problems to test per model (you can choose your own)",
   "id": "6582c3afb579101b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "v3 = {0, 11, 12, 14, 15, 16, 17, 21, 27, 28, 29, 33, 35, 38, 44, 48, 59, 61, 63, 66, 68, 72, 77, 81, 91, 92, 94, 97, 99, 107, 109, 112, 113, 122, 123, 127, 134, 135, 138, 139, 149, 150, 154, 160, 161, 163, 169, 170, 172, 176, 179, 182, 188, 196, 198, 206, 211, 212, 214, 217, 218, 222, 223, 226, 231, 235, 240, 242, 247, 253, 258, 265, 274, 275, 285, 291, 294, 303, 306, 308, 314, 316, 317, 320, 321, 329, 333, 341, 342, 343, 344, 348, 350, 355, 360, 361, 362, 374, 376, 377, 380, 384, 390, 394, 396, 397, 398, 399}\n",
    "\n",
    "\n",
    "\n",
    "v4 = {7, 12, 140, 24, 154, 155, 159, 162, 41, 169, 172, 49, 185, 69, 205, 208, 209, 213, 89, 94, 222, 103, 107, 116, 118, 120, 249}\n",
    "v1 = {191, 258, 3, 103, 169, 208, 81, 209, 116, 213, 150, 254, 24, 185, 154, 222, 159}\n",
    "ca = {7, 136, 140, 142, 24, 159, 162, 41, 169, 172, 49, 185, 63, 191, 69, 71, 205, 209, 213, 89, 94, 223, 103, 231, 235, 118, 120, 254}\n",
    "test = {0}"
   ],
   "id": "ffbeb1e459539597",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Make Videos ",
   "id": "ac96b3149382ab73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for problem in v3:\n",
    "\n",
    "\n",
    "    mode = \"rgb\"\n",
    "    genes = [i for i in range(GENESIZE)]\n",
    "    nca_in = [aau.pad_to_size([32,32],aau.arc_to_nca_space(max_colors, ip, CHANNELS,GENESIZE, mode=mode, gene_location=genes, is_invis=1)) for id,ip in enumerate(inputs[problem])]\n",
    "    nca_out = [aau.pad_to_size([32,32], aau.arc_to_nca_space(max_colors, ip, CHANNELS,GENESIZE, mode=mode, gene_location=genes, is_invis=1)) for id,ip in enumerate(outputs[problem])]\n",
    "\n",
    "    eval_nca_in = [aau.pad_to_size([32,32], aau.arc_to_nca_space(max_colors, ip, CHANNELS,GENESIZE, mode=mode, gene_location=genes, is_invis=1)) for id,ip in enumerate(eval_in[problem])]\n",
    "    eval_nca_out = [aau.pad_to_size([32,32], aau.arc_to_nca_space(max_colors, ip, CHANNELS,GENESIZE, mode=mode, gene_location=genes, is_invis=1)) for id,ip in enumerate(eval_out[problem])]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        nca = EngramNCA_v3(CHANNELS, vft.GENE_HIDDEN_N ,vft.GENE_PROP_HIDDEN_N, GENESIZE)\n",
    "        nca.load_state_dict(torch.load(f\"TrainedARCModels/{type(nca).__name__}/problem_\" +str(problem)+\".pth\"))\n",
    "        nca = nca.to(DEVICE)\n",
    "\n",
    "    nca.eval()\n",
    "\n",
    "\n",
    "    image_np = aau.nca_to_rgb_image(eval_nca_in[0])[:,:,:4]*255\n",
    "    image_np = cv2.resize(image_np, (10 * image_np.shape[1], 10 * image_np.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imwrite(\"Saved_frames/Saved_start_and_end/\"  + type(nca).__name__ +\"problem_start_\" + str(problem)+\".jpg\", image_np)\n",
    "\n",
    "    image_np = aau.nca_to_rgb_image(eval_nca_out[0])[:, :, :4] * 255\n",
    "    image_np = cv2.resize(image_np, (10 * image_np.shape[1], 10 * image_np.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imwrite(\"Saved_frames/Saved_start_and_end/\" + type(nca).__name__ + \"problem_end_\" + str(problem) + \".jpg\",\n",
    "                image_np)\n",
    "\n",
    "\n",
    "    x = eval_nca_in[0].tile(1,1,1,1)\n",
    "\n",
    "    for i in range(128):\n",
    "        x = nca(x, 0.5)\n",
    "        x = x.detach()\n",
    "        write_frame(x, path_video, i, 10*x.shape[3],10*x.shape[2], CHANNELS)\n",
    "\n",
    "    make_video(path_video, 128, 10*x.shape[3],10*x.shape[2], type(nca).__name__ +\"problem_\" + str(problem) +\"padded\")"
   ],
   "id": "a5fb4bb14c306ab4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
