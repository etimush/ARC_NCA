{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "#from Gene_CA_Arc_priors import scheduler\n",
    "from NCA import *\n",
    "import utils\n",
    "from IPython.display import Image, HTML, clear_output\n",
    "import arc_agi_utils as aau\n",
    "import random\n",
    "import vft\n",
    "from utils import make_path, create_empty_json"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Settings",
   "id": "d336d071fabc17b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DEVICE = vft.DEVICE\n",
    "CHANNELS = vft.CHANNELS\n",
    "BATCH_SIZE = vft.BATCH_SIZE\n",
    "MASKING = vft.MASKING\n",
    "GENESIZE = vft.GENESIZE"
   ],
   "id": "567c44a1eba63c78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load ARC-AGI problems and configure number of problems to train",
   "id": "6e3e9ec043deae2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_path = \"ArcData/data/training\"   # Make sure you have the ARC-AGI dta set in the same folder as this file and change the path\n",
    "eval_path = \"ArcData/data/evaluation\"     # Make sure you have the ARC-AGI dta set in the same folder as this file and change the path\n",
    "(inputs, outputs), (eval_in, eval_out)= aau.import_data(training_path)\n",
    "\n",
    "\n",
    "max_train = aau.max_n_colors(inputs, outputs)\n",
    "max_eval = aau.max_n_colors(eval_in, eval_out)\n",
    "max_colors = max(max_train, max_eval) + 1\n",
    "start =0\n",
    "num_probs = 3    #-1 for all problems\n",
    "patch_training = False #wether to use patch training (EngramNCA v4)\n",
    "\n",
    "if num_probs == -1:\n",
    "    num_probs = len(inputs)\n"
   ],
   "id": "846b3106843a98ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train on all selected problem",
   "id": "8ff8eadf59fb2c59"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(start,num_probs):\n",
    "    make_path(\"LossesData\")\n",
    "    name_loss = \"LossesData/v3_padded_loss.json\"    #<--- change name here for new loss saving path\n",
    "    create_empty_json(name_loss)\n",
    "    with open(name_loss, \"r\") as f:\n",
    "        all_problems_loss_history = json.load(f)\n",
    "    name_eval = \"LossesData/v3_padded_eval.json\"    #<--- change name here for new eval saving path\n",
    "    create_empty_json(name_eval)\n",
    "    with open(name_eval, \"r\") as f:\n",
    "        all_problems_eval_history = json.load(f)\n",
    "\n",
    "    problem = i\n",
    "    mode = \"rgb\"\n",
    "    genes = [i for i in range(GENESIZE)]\n",
    "\n",
    "    nca_in = [aau.pad_to_size([32,32],aau.arc_to_nca_space(max_colors, ip, CHANNELS,GENESIZE, mode=mode, gene_location=genes, is_invis=1)) for id,ip in enumerate(inputs[problem])]\n",
    "    nca_out = [aau.pad_to_size([32,32], aau.arc_to_nca_space(max_colors, ip, CHANNELS,GENESIZE, mode=mode, gene_location=genes, is_invis=1)) for id,ip in enumerate(outputs[problem])]\n",
    "\n",
    "    eval_nca_in = [aau.pad_to_size([32,32], aau.arc_to_nca_space(max_colors, ip, CHANNELS,GENESIZE, mode=mode, gene_location=genes, is_invis=1)) for id,ip in enumerate(eval_in[problem])]\n",
    "    eval_nca_out = [aau.pad_to_size([32,32], aau.arc_to_nca_space(max_colors, ip, CHANNELS,GENESIZE, mode=mode, gene_location=genes, is_invis=1)) for id,ip in enumerate(eval_out[problem])]\n",
    "\n",
    "    pool_x = [n.tile(1024, 1, 1, 1) for n in nca_in]\n",
    "    xprime = nca_in\n",
    "    pool_y = nca_out\n",
    "    folder = \"TrainedARCModels\"\n",
    "\n",
    "    eval_log = []\n",
    "    loss_log = []\n",
    "    nca = EngramNCA_v3(CHANNELS, vft.GENE_HIDDEN_N, vft.GENE_PROP_HIDDEN_N, GENESIZE)  #<-- change CA here {options: CA, EngramNCA v1, v2 , v3 , v3}\n",
    "\n",
    "    make_path(folder+ \"/\" + type(nca).__name__)\n",
    "\n",
    "    nca = nca.to(DEVICE)\n",
    "    optim = torch.optim.AdamW(nca.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=2000, gamma=0.3)\n",
    "    name_NCA = folder + \"/\" + type(nca).__name__ + \"/\" + \"problem_\" + str(i) + \".pth\"\n",
    "\n",
    "    for j in range(3000):\n",
    "        nca.train()\n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            idx_problem = j % len(pool_x)\n",
    "            x_prob = pool_x[idx_problem]\n",
    "            x, idxs = utils.get_batch(x_prob, nca_in[idx_problem].clone(), BATCH_SIZE, noise_level=0.2)\n",
    "            y = pool_y[idx_problem].tile(BATCH_SIZE, 1, 1, 1)\n",
    "\n",
    "        itters = random.randrange(32, 64)\n",
    "\n",
    "        for _ in range(itters):\n",
    "            x = nca(x, 0.5)\n",
    "        loss += ((y[:, :4, :, :]) - (x[:, :4, :, :])).pow(2).mean()\n",
    "\n",
    "        if patch_training:\n",
    "            xp = torch.nn.functional.unfold(x, kernel_size=3, padding=1, stride=1).view(\n",
    "                BATCH_SIZE * x.shape[-1] * x.shape[-2], CHANNELS, 3, 3)\n",
    "            yp = torch.nn.functional.unfold(y, kernel_size=3, padding=1, stride=1).view(\n",
    "                BATCH_SIZE * y.shape[-1] * y.shape[-2], CHANNELS, 3, 3)\n",
    "            indexes = random.sample(range(xp.shape[0]), xp.shape[0] // 50)\n",
    "            xp = xp[indexes]\n",
    "            yp = yp[indexes]\n",
    "\n",
    "            for _ in range(itters):\n",
    "                xp = nca(xp, 0.5)\n",
    "\n",
    "            loss += (yp[:, :4, :, :] - (xp[:, :4, :, :])).pow(2).mean()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss.backward()\n",
    "            for p in nca.parameters():\n",
    "                p.grad /= (p.grad.norm() + 1e-8)\n",
    "            optim.step()\n",
    "            x = x.detach()\n",
    "            optim.zero_grad()\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pool_x = utils.update_problem_pool(pool_x, x.clone(), idxs, idx_problem)\n",
    "        loss_log.append(loss.log().item() if loss.log().item() < 15 else 15)\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "        nca.eval()\n",
    "        with torch.no_grad():\n",
    "            xe = eval_nca_in[0].clone().tile(BATCH_SIZE, 1, 1, 1)\n",
    "            ye = eval_nca_out[0].clone().tile(BATCH_SIZE, 1, 1, 1)\n",
    "            for _ in range(itters):\n",
    "                xe = nca(xe, 0.5)\n",
    "            loss_e = (ye[:, :4, :, :] - xe[:, :4, :, :]).pow(2).mean()\n",
    "            eval_log.append(loss_e.log().item() if loss_e.log().item() < 15 else 15)\n",
    "            loss_e.detach()\n",
    "            xe.detach()\n",
    "            ye.detach()\n",
    "\n",
    "        if j % 101 == 0:\n",
    "            plt.figure(1, figsize=(10, 4))\n",
    "            plt.clf()\n",
    "            clear_output()\n",
    "            plt.title('Loss history')\n",
    "            plt.plot(loss_log, '.', alpha=0.5, color=\"b\")\n",
    "            plt.figure(2, figsize=(10, 4))\n",
    "            plt.clf()\n",
    "            clear_output()\n",
    "            plt.title('Eval history')\n",
    "            plt.plot(eval_log, '.', alpha=0.5, color=\"b\")\n",
    "            plt.show(block=False)\n",
    "            plt.pause(0.01)\n",
    "            utils.show_batch(y[0:8], fig_num=6, channels=4)\n",
    "            utils.show_batch(x[0:8],channels=4)\n",
    "            utils.show_batch(ye[0:8],fig_num=5,channels=4)\n",
    "            utils.show_batch(xe[0:8],fig_num=4,channels=4)\n",
    "\n",
    "    print(f\"Loss for problem {i}: {sum(eval_log[-10:])/10}\")\n",
    "\n",
    "\n",
    "    all_problems_loss_history[str(i)] = loss_log\n",
    "    all_problems_eval_history[str(i)] = eval_log\n",
    "    torch.save(nca.state_dict(), name_NCA)\n",
    "\n",
    "\n",
    "    with open(name_loss, \"w\") as f:\n",
    "        json.dump(all_problems_loss_history, f)\n",
    "\n",
    "\n",
    "    with open(name_eval, \"w\") as f:\n",
    "        json.dump(all_problems_eval_history, f)"
   ],
   "id": "8d263e2f1ec7c2ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fabc257dd9ae0b5e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
